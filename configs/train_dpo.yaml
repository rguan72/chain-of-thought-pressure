# DPO Training configuration

dpo:
  # LoRA configuration (initialized from SFT)
  lora:
    r: 64
    lora_alpha: 128
    lora_dropout: 0.05
    target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    bias: "none"
    task_type: "CAUSAL_LM"

  # DPO-specific parameters
  beta_values: [0.1, 0.3]  # DPO temperature

  # Training parameters
  training:
    epochs: 2
    batch_size: 4
    gradient_accumulation_steps: 8
    learning_rate: 1.0e-5
    warmup_ratio: 0.1
    max_seq_length: 2048
    max_prompt_length: 1024

  output:
    checkpoint_dir: "runs/dpo"
